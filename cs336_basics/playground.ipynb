{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45582dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u don't have to be scared of the loud dog, I'll protect you\". The mole felt so safe with the little girl. She was very kind and the mole soon came to trust her. He leaned against her and she kept him safe. The mole had found his best friend.\n",
      "<|endoftext|>\n",
      "Once upon a time, in a warm and sunny place, there was a big pit. A little boy named Tom liked to play near the pit. One day, Tom lost his red ball. He was very sad.\n",
      "Tom asked his friend, Sam, to help him search for the ball. They looked high and low, but they could not find the ball. Tom said, \"I think my ball fell into the pit.\"\n",
      "Sam and Tom went close to the pit. They were scared, but they wanted to find the red ball. They looked into the pit, but it was too dark to see. Tom said, \"We must go in and search for my ball.\"\n",
      "They went into the pit to search. It was dark and scary. They could not find the ball. They tried to get out, but the pit was too deep. Tom and Sam were stuck in the pit. They called for help, but no one could hear them. They were sad and scared, and they never got out of the pit.\n",
      "<|endoftext|>\n",
      "\n",
      "\n",
      "Tom and Lily were playing with their toys in the living room. They liked to build towers and bridges with their blocks and cars. Tom was very proud of his tall tower. He wanted to make it even taller, so he reached for more blocks.\n",
      "\"Tom, can I have some blocks too?\" Lily asked. She wanted to make a bridge for her cars.\n",
      "\"No, these are mine. Go find your own,\" Tom said. He did not want to share with his sister. He pulled the blocks closer to him.\n",
      "Lily felt sad and angry. She did not think Tom was being nice. She looked at his tower and had an idea. She decided to pull one of the blocks at the bottom of the tower.\n",
      "Suddenly, the tower fell down with a loud crash. All the blocks and cars scattered on the floor. Tom and Lily were shocked. They felt the floor shake and heard a rumble. It was an earthquake!\n",
      "\"Mommy! Daddy!\" they cried. They were scared and ran to their parents, who were in the kitchen.\n",
      "\"Are you okay, kids?\" Mommy asked. She hugged them and checked if they were hurt.\n",
      "\"We're okay, Mommy. But our toys are broken,\" Lily said.\n",
      "\"I'm sorry, Lily. But toys are not important. You are important. We are safe and together. That's what matters,\" Mommy said.\n",
      "Tom felt sorry for what he did. He realized he was selfish and mean to his sister. He saw how scared she was during the earthquake. He wanted to make her happy.\n",
      "\"Lily, I'm sorry I did not share with you. You can have all the blocks you want. I love you, sister,\" Tom said.\n",
      "Lily smiled and hugged him. She forgave him and thanked him. She loved him too.\n",
      "They went back to the living room and cleaned up their toys. They decided to build something together. They made a big house with a garden and a fence. They put their cars and dolls inside. They were happy and proud of their work.\n",
      "Mommy and Daddy came to see their house. They praised them and gave them a treat. It was a lemon cake. It was sour, but they liked it. They learned that sharing is caring, and that family is sweet.\n",
      "\n",
      "Chunk size: 3035 bytes\n"
     ]
    }
   ],
   "source": [
    "# test chunking\n",
    "\n",
    "import os\n",
    "from typing import BinaryIO\n",
    "from find_chunk_boundaries import find_chunk_boundaries\n",
    "\n",
    "TEST_FILE = os.path.join(\"..\", \"data\", \"TinyStoriesV2-GPT4-valid.txt\")\n",
    "SPLIT_TOKEN = b\"<|endoftext|>\"\n",
    "\n",
    "def test_find_chunk_boundaries():\n",
    "    with open(TEST_FILE, \"rb\") as f:\n",
    "        boundaries = find_chunk_boundaries(f, desired_num_chunks=20000, split_special_token=SPLIT_TOKEN)\n",
    "    \n",
    "        # output the first chunk\n",
    "        start = boundaries[0]\n",
    "        end = boundaries[1] if len(boundaries) > 1 else None\n",
    "        f.seek(start)\n",
    "        chunk = f.read(end - start if end else None)\n",
    "        print(chunk.decode(\"utf-8\", errors=\"ignore\"))\n",
    "\n",
    "        # check the size of the chunk\n",
    "        chunk_size = end - start if end else os.path.getsize(TEST_FILE) - start\n",
    "        print(f\"Chunk size: {chunk_size} bytes\")\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_find_chunk_boundaries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba8d9077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First chunk size: 3035 bytes\n",
      "Number of tokens in first chunk: 256\n",
      "First 10 tokens: [((b'u',), 1), ((b' don',), 1), ((b\"'t\",), 1), ((b' have',), 3), ((b' to',), 21), ((b' be',), 1), ((b' scared',), 5), ((b' of',), 6), ((b' the',), 28), ((b' loud',), 2)]\n",
      "Top 10 most frequent pretokens:\n",
      "Pretoken: (b'.',), Count: 69\n",
      "Pretoken: (b' and',), Count: 31\n",
      "Pretoken: (b' the',), Count: 28\n",
      "Pretoken: (b',',), Count: 26\n",
      "Pretoken: (b' to',), Count: 21\n",
      "Pretoken: (b'\\n',), Count: 21\n",
      "Pretoken: (b' They',), Count: 16\n",
      "Pretoken: (b' was',), Count: 13\n",
      "Pretoken: (b' a',), Count: 11\n",
      "Pretoken: (b' Tom',), Count: 11\n"
     ]
    }
   ],
   "source": [
    "# test the pretonizer on the first chunk\n",
    "\n",
    "import os\n",
    "from typing import BinaryIO\n",
    "from find_chunk_boundaries import find_chunk_boundaries\n",
    "from pretokenization import pretokenize\n",
    "\n",
    "TEST_FILE = os.path.join(\"..\", \"data\", \"TinyStoriesV2-GPT4-valid.txt\")\n",
    "SPLIT_TOKEN = b\"<|endoftext|>\"\n",
    "\n",
    "def test_pretokenize_first_chunk():\n",
    "    with open(TEST_FILE, \"rb\") as f:\n",
    "        boundaries = find_chunk_boundaries(f, desired_num_chunks=20000, split_special_token=SPLIT_TOKEN)\n",
    "        \n",
    "        # read the first chunk\n",
    "        start = boundaries[0]\n",
    "        end = boundaries[1] if len(boundaries) > 1 else None\n",
    "        f.seek(start)\n",
    "        chunk = f.read(end - start if end else None)\n",
    "        print(f\"First chunk size: {len(chunk)} bytes\")\n",
    "        \n",
    "        # pretokenize the chunk\n",
    "        tokens = pretokenize(chunk.decode(\"utf-8\"), special_tokens=[SPLIT_TOKEN.decode(\"utf-8\")])\n",
    "        print(f\"Number of tokens in first chunk: {len(tokens)}\")\n",
    "        print(f\"First 10 tokens: {list(tokens.items())[:10]}\")\n",
    "\n",
    "        # check the most frequent pretokens\n",
    "        sorted_tokens = sorted(tokens.items(), key=lambda item: item[1], reverse=True)\n",
    "        print(\"Top 10 most frequent pretokens:\")\n",
    "        for token, count in sorted_tokens[:10]:\n",
    "            print(f\"Pretoken: {token}, Count: {count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_pretokenize_first_chunk()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
